{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file `../toy_mstis_1k_OPS1.nc` for analysis\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "# if our large test file is available, use it. Otherwise, use file generated from toy_mstis_2_run.ipynb\n",
    "import os\n",
    "test_file = \"../toy_mstis_1k_OPS1.nc\"\n",
    "filename = test_file if os.path.isfile(test_file) else \"mstis.nc\"\n",
    "print(\"Using file `\"+ filename + \"` for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need is contained in the `openpathsampling` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpathsampling as paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The storage itself is mainly a netCDF file and can also be used as such. Technically it is a subclass of `netCDF4.Dataset` and can use all of its functions in case we want to add additional tables to the file besides what we store using stores. You can of course also add new stores to the storage. Using `Storage()` will automatically create a set of needed storages when a new file is created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "netCDF files are very generic while our Storage is more tuned to needs we have. It support etc native support for simtk.units, and can recursively store nested objects using JSON pickling. But we will get to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the output from the 'alanine.ipynb' notebook to have something to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Storage @ '../toy_mstis_1k_OPS1.nc'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage = paths.Storage(filename, mode='r')\n",
    "storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_storage = paths.Storage(\"tmp_storage.nc\", mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and have a look at what stores are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stores', 'trajectories', 'topologies', 'snapshots', 'samples', 'samplesets', 'movechanges', 'steps', 'details', 'pathmovers', 'shootingpointselectors', 'engines', 'pathsimulators', 'transitions', 'networks', 'schemes', 'interfacesets', 'msouters', 'volumes', 'ensembles', 'tag', 'attributes', 'snapshot0', 'cv0', 'cv1', 'cv2']\n"
     ]
    }
   ],
   "source": [
    "print(storage.list_stores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = storage.cvs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18995551764965057,\n",
       " 0.2078997790813446,\n",
       " 0.22507938742637634,\n",
       " 0.24157929420471191,\n",
       " 0.25697261095046997,\n",
       " 0.27420175075531006,\n",
       " 0.2925931513309479,\n",
       " 0.3085809350013733,\n",
       " 0.3231964707374573,\n",
       " 0.3367229104042053]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv(storage.trajectories[0])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can access all of these using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_store = storage.snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stores are lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it is useful to think about the storage as a set of lists. Each of these lists contain objects of the same type, e.g. `Sample`, `Trajectory`, `Ensemble`, `Volume`, ... The class instances used to access elements from the storage are called a store. Imagine you go into a store to *buy* and *sell* objects (luckily our stores are free). All the stores share the same storage space, which is a netCDF file on disc.\n",
    "\n",
    "Still, a store is not really a list or subclassed from a list, but it almost acts like one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 51332 snapshots in our storage\n"
     ]
    }
   ],
   "source": [
    "print('We have %d snapshots in our storage' % len(storage.snapshots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the same way we access lists we can also access these lists using slicing, and even lists of indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load by slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Sample @ 0x11c124c18>, <Sample @ 0x11c154278>]\n"
     ]
    }
   ],
   "source": [
    "print(storage.samples[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load by list of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<openpathsampling.ensemble.TISEnsemble object at 0x11c1328d0>, <openpathsampling.ensemble.IntersectionEnsemble object at 0x11caa25c0>, <openpathsampling.ensemble.LengthEnsemble object at 0x11caa2630>]\n"
     ]
    }
   ],
   "source": [
    "print(storage.ensembles[[0,1,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving is somehow special, since we try to deal exclusively with immutable objects. That means that once an object is saved, it cannot be changed. This is not completely true, since the netCDF file allow changing, but we try not to do it. The only exeption are collective variables, these can store their cached values and we want to store intermediate states so we add new values once we have computed these. This should be the only exception and we use the `.sync` command to update the status of a once saved collectivevariable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving is easy. Just use `.save` on the store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage.samples.save(my_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it will add the object to the end of our store list or do nothing, if the object has already been stored. It is important to note, that each object knows, if it has been stored already. This allows to write nice recursive saving without worrying that we save the same object several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also store directly using the storage. Both is fine and the storage just delegates the task to the appropriate store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage.save(my_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness you can also use `__setitem__` to save, but since you cannot explicitely set the number you have to use `None` as the key or `Ellipsis`, `...` is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage.samples[None] = my_sample\n",
    "# storage.samples[Ellipsis] = my_sample\n",
    "# storage.samples[...] = my_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned recursive saving. This does the following. Imagine a sample `snapshot` which itself has a `Configuration` and a `Momentum` object. If you store the snapshot it also store the content using the approriate stores. This can be arbitrarily complex. And most object can be either stored in a special way or get converted into a JSON string that we can turn into an object again. Python has something like this build it, which works similar, but we needed something that add the recursive storage connection and uses JSON. If you are curious, the json string can be accessed for some objects using `.json` but is only available for loaded or saved objects. It will not be computed unless it is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_cls\":\"UnionVolume\",\"_dict\":{\"volume1\":{\"_hex_uuid\":\"0x833357f8dd4b11e7b495000000000022\",\"_store\":\"volumes\"},\"volume2\":{\"_hex_uuid\":\"0x833357f8dd4b11e7b495000000000024\",\"_store\":\"volumes\"}}}\n"
     ]
    }
   ],
   "source": [
    "volume = storage.volumes[1]\n",
    "print(storage.repr_json(volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "print(storage.volumes['A'].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to using `.find(<name>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "print(storage.volumes.find('A').name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names can exist multiple times. To find all use `.find_all()` which returns a list of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<openpathsampling.volume.CVDefinedVolume object at 0x11c132dd8>]\n"
     ]
    }
   ],
   "source": [
    "print(storage.volumes.find_all('A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get indices only use `.find_indices(<name>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(storage.volumes.find_indices('A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the internal list of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {0},\n",
       " 'all states except A': {1},\n",
       " 'B': {2},\n",
       " 'C': {3},\n",
       " 'all states except B': {13},\n",
       " 'all states except C': {23}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.volumes.name_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects can be saved by name. To do this we need a new objects that we can actually save. All loaded objects cannot be saved again of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_ensemble = paths.EmptyEnsemble()\n",
    "full_ensemble = paths.EmptyEnsemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now store as you would set a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(storage.ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_storage.ensembles['empty'] = empty_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(save_storage.ensembles.index[empty_ensemble.__uuid__])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['{\"_cls\":\"TISEnsemble\",\"_dict\":{\"min_overlap\":{\"_tuple\":[0,0]},\"initial_states\":[{\"_hex_uuid\":\"0x833357f8dd4b11e7b495000000000024\",\"_store\":\"volumes\"}],\"final_states\":[{\"_hex_uuid\":\"0x9ad8903add4b11e7a11600000000042e\",\"_store\":\"volumes\"}],\"max_overlap\":{\"_tuple\":[0,0]},\"interface\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a1160000000002ba\",\"_store\":\"volumes\"},\"orderparameter\":{\"_hex_uuid\":\"0x833357f8dd4b11e7b49500000000001e\",\"_store\":\"attributes\"},\"lambda_i\":null,\"ensembles\":[{\"_hex_uuid\":\"0x9ad8903add4b11e7a116000000000458\",\"_store\":\"ensembles\"},{\"_hex_uuid\":\"0x9ad8903add4b11e7a116000000000460\",\"_store\":\"ensembles\"},{\"_hex_uuid\":\"0x9ad8903add4b11e7a116000000000468\",\"_store\":\"ensembles\"}],\"greedy\":false}}',\n",
       "       '{\"_cls\":\"IntersectionEnsemble\",\"_dict\":{\"ensemble1\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a116000000000454\",\"_store\":\"ensembles\"},\"ensemble2\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a116000000000456\",\"_store\":\"ensembles\"}}}',\n",
       "       '{\"_cls\":\"AllInXEnsemble\",\"_dict\":{\"trusted\":true,\"volume\":{\"_hex_uuid\":\"0x833357f8dd4b11e7b495000000000024\",\"_store\":\"volumes\"}}}',\n",
       "       '{\"_cls\":\"LengthEnsemble\",\"_dict\":{\"length\":1}}',\n",
       "       '{\"_cls\":\"IntersectionEnsemble\",\"_dict\":{\"ensemble1\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a11600000000045c\",\"_store\":\"ensembles\"},\"ensemble2\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a11600000000045e\",\"_store\":\"ensembles\"}}}',\n",
       "       '{\"_cls\":\"AllOutXEnsemble\",\"_dict\":{\"trusted\":true,\"volume\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a11600000000045a\",\"_store\":\"volumes\"}}}',\n",
       "       '{\"_cls\":\"PartOutXEnsemble\",\"_dict\":{\"trusted\":true,\"volume\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a1160000000002ba\",\"_store\":\"volumes\"}}}',\n",
       "       '{\"_cls\":\"IntersectionEnsemble\",\"_dict\":{\"ensemble1\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a116000000000464\",\"_store\":\"ensembles\"},\"ensemble2\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a116000000000466\",\"_store\":\"ensembles\"}}}',\n",
       "       '{\"_cls\":\"AllInXEnsemble\",\"_dict\":{\"trusted\":true,\"volume\":{\"_hex_uuid\":\"0x9ad8903add4b11e7a116000000000462\",\"_store\":\"volumes\"}}}',\n",
       "       '{\"_cls\":\"LengthEnsemble\",\"_dict\":{\"length\":1}}'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.variables['ensembles_json'][70:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can use save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224870977374347836148056640146643091776"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_storage.ensembles.save(full_ensemble, 'full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ensemble now has the `.name` property set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n",
      "full\n"
     ]
    }
   ],
   "source": [
    "print(empty_ensemble.name)\n",
    "print(full_ensemble.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In storage exists a special store name `tag`. This is to reference any object and mostly to name stuff for later easy access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(storage.tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in storage.tag:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.tag.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, obj in storage.tag.iteritems():\n",
    "    print('{:20s} : {:20s}'.format(name, obj.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(storage.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loaded object is equipped with a `.idx` attribute which is a dictionary that contains the index for a specific storage. This is necessary since we can - in theory - store an object in several different stores at once and these might have different indices. Note that idx is NOT a function, but a dictionary, hence the square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "samp = storage.samples[0]\n",
    "print(storage.idx(samp))\n",
    "print(samp.idx(storage))\n",
    "print(samp.idx(storage.samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list is iterable and so is a store. Lets load all ensembles and list their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[AllInXEnsemble]', '[LengthEnsemble]']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ens.name for ens in storage.ensembles][2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you have realized that some command run slower the first time. This is because we use caching and once an object is loaded it stays in memory and can be accessed much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to find objects is to use their name, which I mentioned before, but in general there are no search functions, but we can use python notation in the usual way to load what we need. *List comprehensions* is the magic word.\n",
    "Say, we want to get all snapshots that are reversed. We could just load all of these and filter them, but there is a more elegant way to do that, or let's say a more elegant way of writing it in python, because the underlying code does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 572 snapshots in StateA among 5000 total snapshots\n"
     ]
    }
   ],
   "source": [
    "stA = storage.volumes['A']\n",
    "first_5000_snaps = storage.snapshots[0:5000]\n",
    "reversed_samples = [snapshot for snapshot in first_5000_snaps if stA(snapshot)]\n",
    "print('We found %d snapshots in StateA among %d total snapshots' % (len(reversed_samples), len(first_5000_snaps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do something more useful: For TIS ensemble we want statistics on pathlengths associated with sampled trajectories `Sample` objects that are sampled for a specific ensemble. And we one want samples that have been generated in our production runs and are present in a `SampleSet`\n",
    "\n",
    "> TODO: add a way to select only specific SampleSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openpathsampling.sample.SampleSet object at 0x11cac5ef0>\n"
     ]
    }
   ],
   "source": [
    "print(storage.samplesets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "my_network = storage.networks[0]\n",
    "my_ensemble = my_network.sampling_ensembles[0]\n",
    "relevant_samples = [\n",
    "    sample \n",
    "    for sample_set in storage.samplesets \n",
    "    for sample in sample_set \n",
    "    if sample.ensemble is my_ensemble\n",
    "]\n",
    "print(len(relevant_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally compute the average length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 44, 31, 31, 31, 31, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 46, 46, 46, 46, 46, 46, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 75, 75, 69, 69, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 26, 26, 26, 26, 26, 26, 26, 26, 26, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "20.195804195804197\n"
     ]
    }
   ],
   "source": [
    "list_of_path_lengths = [\n",
    "    len(sample.trajectory)\n",
    "    for sample_set in storage.samplesets \n",
    "    for sample in sample_set \n",
    "    if sample.ensemble is my_ensemble\n",
    "]\n",
    "print(list_of_path_lengths)\n",
    "if len(list_of_path_lengths) > 0:\n",
    "    mean = float(sum(list_of_path_lengths))/len(list_of_path_lengths)\n",
    "else:\n",
    "    mean = 0.0 # actually, it is not defined, so we just set it to zero\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, we loaded from a bootstrapping sampling algorithm and the analysis is pointless, but still it is rather short considering what we just did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another very cool feature about python that is worth noting: generator expressions. Before we used list comprehensions to generate a list of all that we need, but what, if we don't want the whole list at once? Maybe that is impossible because of too much memory and also not desirable? We can do the same thing as above using a generator (although it would only be useful if we had to average over billions of samples). So assume the list of lengths is too large for memory. The summing does not mind to use little pieces so we construct a function that always gives us the next element. These functions are called iterators and to make these iteratore there is syntactic way to create them easily: Instead of square brackets in in list comprehensions use round brackets. So the above example would look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20216.0\n"
     ]
    }
   ],
   "source": [
    "iterator_over_path_lengths = (\n",
    "    len(sample.trajectory)\n",
    "    for sample_set in storage.samplesets\n",
    "    for sample in sample_set \n",
    "    if sample.ensemble is my_ensemble\n",
    ")\n",
    "total = float(sum(iterator_over_path_lengths))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have a generator and no computed values yet. If we iterator using our iterator called generator it will pass one value at a time and we can use it in sum as we did before. There are two important things to note. Once an iteratore is used, it is consumed and we cannot just be run again so we need to change the code again. I assume there are other ways to do that, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.195804195804197\n"
     ]
    }
   ],
   "source": [
    "iterator_over_path_lengths = (\n",
    "    len(sample.trajectory)\n",
    "    for sample_set in storage.samplesets \n",
    "    for sample in sample_set \n",
    "    if sample.ensemble is my_ensemble\n",
    ")\n",
    "total = 0\n",
    "count = 0\n",
    "for length in iterator_over_path_lengths:\n",
    "    total += length\n",
    "    count += 1\n",
    "    \n",
    "if count > 0:\n",
    "    mean = float(total)/count\n",
    "else:\n",
    "    mean = 0.0 # actually, it is not defined, so we just set it to zero\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voil√†, this time without computing all length before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last example that will be interesting is the statistics on acceptance. Each sample knows which mover was involved in its creation. This is stored in `.details.mover` in the `.details` attribute. Let us try to look at only forward moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_movers = list(filter(lambda self : type(self) == paths.ForwardShootMover, storage.pathmovers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<openpathsampling.pathmover.ForwardShootMover at 0x11dba4898>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11d312c88>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11db53080>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11d493588>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11d493cc0>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11dcb2898>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11d493ba8>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11d477d68>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11db53128>,\n",
       " <openpathsampling.pathmover.ForwardShootMover at 0x11d493860>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_movers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use a 'ForwardShootMover' for ensemble(s) 'Out A 2'\n"
     ]
    }
   ],
   "source": [
    "if len(ff_movers) > 2:\n",
    "    mover = ff_movers[2]\n",
    "    print(\"Use a '%s' for ensemble(s) '%s'\" % ( mover.cls, mover.ensemble.name ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a little trick here, notice that we use a list comprehension inside of a function call, this actually uses the generator expression and passes the resulting iterator to the `.join` function.\n",
    "\n",
    "Now to get statistics on acceptances"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
